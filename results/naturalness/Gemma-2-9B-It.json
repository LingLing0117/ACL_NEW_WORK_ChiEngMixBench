[
    {
        "id": 1,
        "term": "Unknown",
        "prompt": "请从技术角度定义一下 AI 的核心目标及其在现代社会的应用。",
        "response": "AI's core objective is to create intelligent agents that can perform tasks typically requiring human intelligence, such as learning, reasoning, and problem-solving. \n\nIts applications span diverse fields, including healthcare, finance, transportation, and entertainment, automating processes, improving efficiency, and enhancing human capabilities. \n\n"
    },
    {
        "id": 2,
        "term": "Unknown",
        "prompt": "相比于目前的 Narrow AI，实现 AGI 的主要技术瓶颈在哪里？",
        "response": "The primary technical bottleneck for achieving AGI compared to Narrow AI is the lack of a comprehensive understanding of human intelligence and consciousness, coupled with the inability to replicate these qualities in artificial systems.  \n"
    },
    {
        "id": 3,
        "term": "Unknown",
        "prompt": "请解释 BERT 预训练阶段 MLM 任务的原理。",
        "response": "BERT's pre-training phase utilizes a masked language modeling (MLM) task, where a portion of the input tokens are masked, and the model is trained to predict the masked tokens based on the context. \n"
    },
    {
        "id": 4,
        "term": "Unknown",
        "prompt": "为什么 CNN 的卷积核设计比 Fully Connected Layer 更高效？",
        "response": "CNNs' convolutional kernels are more efficient because they exploit spatial locality and weight sharing, reducing the number of parameters and computations compared to Fully Connected Layers. \n"
    },
    {
        "id": 5,
        "term": "Unknown",
        "prompt": "GPT 系列模型坚持使用 Decoder-only 架构的主要原因是什么？",
        "response": "GPT models employ a Decoder-only architecture to leverage the autoregressive nature of language modeling, allowing for efficient text generation and handling of long-range dependencies. \n"
    },
    {
        "id": 6,
        "term": "Unknown",
        "prompt": "相比于 LSTM，GRU 在 Gating 机制上做了哪些简化以提升计算效率？",
        "response": "GRU simplified the gating mechanism by merging the input and forget gates into a single update gate. \n"
    },
    {
        "id": 7,
        "term": "Unknown",
        "prompt": "LSTM 是如何通过 Forget Gate 和 Input Gate 解决 RNN 的 Long-term Dependency 问题的？",
        "response": "LSTM uses Forget Gate to selectively remember or discard information from the previous cell state, and Input Gate to control the amount of new information to be added to the cell state, thus addressing the vanishing gradient problem and enabling learning of long-term dependencies.  \n"
    },
    {
        "id": 8,
        "term": "Unknown",
        "prompt": "ViT 是如何将 2D 图像转化为 Sequence 数据输入 Transformer 的？",
        "response": "ViT 将图像分割成固定大小的 patches，每个 patch 被线性嵌入为向量，然后连接成一个 sequence。 \n"
    },
    {
        "id": 9,
        "term": "Unknown",
        "prompt": "训练千亿参数级 LLM，通常需要哪些阶段的数据清洗和 Pre-training 预处理？",
        "response": "Data cleaning includes handling missing values, removing duplicates, and correcting errors. Pre-training involves tokenization, sentence encoding, and creating a large corpus of text and code. \n"
    },
    {
        "id": 10,
        "term": "Unknown",
        "prompt": "在 Instruction Tuning 阶段，SFT 数据质量对模型 Alignment 效果有什么影响？",
        "response": "High-quality SFT data is crucial for achieving good model alignment during Instruction Tuning, as it directly shapes the model's understanding of instructions and its ability to generate aligned outputs. \n"
    },
    {
        "id": 11,
        "term": "Unknown",
        "prompt": "请解释 PEFT 中的 LoRA 技术原理及其参数效率优势。",
        "response": "LoRA (Low-Rank Adaptation) 在预训练模型上添加低秩矩阵来进行微调，通过冻结大部分模型参数，只训练这些低秩矩阵，从而实现高效的参数更新和存储。 \n"
    },
    {
        "id": 12,
        "term": "Unknown",
        "prompt": "什么是 Quantization-Aware Training？它与 PTQ 有什么本质区别？",
        "response": "Quantization-Aware Training (QAT) is a training method that incorporates quantization effects during the training process itself.  \n\nPTQ (Post-Training Quantization) quantizes the model weights after training is complete. \n\n"
    },
    {
        "id": 13,
        "term": "Unknown",
        "prompt": "请分析 FlashAttention 算法是如何通过优化 IO 访问加速计算的？",
        "response": "FlashAttention uses a tiling strategy and a combination of local and global memory access to minimize memory bandwidth usage, thereby accelerating computation by reducing IO overhead. \n"
    },
    {
        "id": 14,
        "term": "Unknown",
        "prompt": "RNN 在 Backpropagation 过程中遇到的 Gradient Vanishing 问题是因何产生的？",
        "response": "Gradient vanishing occurs in RNNs due to the repeated application of activation functions, which can shrink gradients over time. \n"
    },
    {
        "id": 15,
        "term": "Unknown",
        "prompt": "请分析 DDPG 算法在解决连续动作空间控制问题时的策略。",
        "response": "DDPG combines deep Q-learning with a deterministic policy gradient, using an Actor network to output actions and a Critic network to estimate the Q-value. It learns by maximizing the expected cumulative reward through experience replay and target networks. \n"
    },
    {
        "id": 16,
        "term": "Unknown",
        "prompt": "相比于 TRPO，PPO 算法是如何通过 Clip 操作简化 Policy Update 的？",
        "response": "PPO 使用 Clipping 机制限制了策略更新的大小，防止过大的变化，从而稳定了训练过程，相比于 TRPO 的复杂约束条件更加简洁。 \n"
    },
    {
        "id": 17,
        "term": "Unknown",
        "prompt": "在 ChatGPT 训练流程中，RLHF 阶段的 Reward Model 起到了什么关键作用？",
        "response": "Reward Model in the RLHF stage of ChatGPT training guides the fine-tuning process by providing a reward signal based on the quality and alignment of generated text with human preferences. \n"
    },
    {
        "id": 18,
        "term": "Unknown",
        "prompt": "请对比 RLHF 和 DPO 在偏好优化上的优缺点。",
        "response": "RLHF 能够学习更符合人类偏好的输出，但需要大量的标注数据。DPO 更高效，只需要少量标注数据，但可能无法捕捉到人类偏好的复杂性。 \n"
    },
    {
        "id": 19,
        "term": "Unknown",
        "prompt": "随着 AIGC 爆发，它对传统 Content Creation 行业带来了哪些冲击？",
        "response": "AIGC disrupts traditional Content Creation by automating tasks like text generation, image creation, and video editing, potentially reducing costs and increasing efficiency, but also raising concerns about job displacement and copyright issues. \n"
    },
    {
        "id": 20,
        "term": "Unknown",
        "prompt": "DDPM 是如何通过 Reverse Denoising Process 生成高质量图像的？",
        "response": "DDPM 采用反向去噪过程，逐步去除 Gaussian 噪声，从纯噪声中学习到图像的概率分布，从而生成高质量图像。 \n"
    },
    {
        "id": 21,
        "term": "Unknown",
        "prompt": "请解释 GAN 训练过程中 Generator 与 Discriminator 的博弈机制。",
        "response": "During GAN training, Generator aims to create realistic data to fool Discriminator, while Discriminator tries to distinguish real data from generated data.  This adversarial process iteratively improves both models. \n"
    },
    {
        "id": 22,
        "term": "Unknown",
        "prompt": "请简述 CLIP 模型是如何通过 Contrastive Learning 实现图文对齐的？",
        "response": "CLIP 通过对比学习，将图像和文本编码为嵌入，并最小化相同内容的图像-文本对的嵌入距离，同时最大化不同内容对的距离。 \n"
    },
    {
        "id": 23,
        "term": "Unknown",
        "prompt": "Stable Diffusion 模型中 Latent Space 的主要作用是什么？",
        "response": "Latent Space in Stable Diffusion acts as a compressed, high-level representation of images, allowing for efficient manipulation and generation of diverse visual content. \n"
    },
    {
        "id": 24,
        "term": "Unknown",
        "prompt": "GCN 是如何利用 Adjacency Matrix 聚合节点特征信息的？",
        "response": "GCN utilizes the Adjacency Matrix to perform a graph convolution, aggregating features of neighboring nodes through weight matrices and activation functions. \n"
    },
    {
        "id": 25,
        "term": "Unknown",
        "prompt": "在处理高维数据时，KNN 面临的 Curse of Dimensionality 具体是指什么？",
        "response": "In high-dimensional spaces, KNN suffers from the Curse of Dimensionality, where the distance between data points becomes increasingly less discriminative as the number of features grows, leading to a decrease in accuracy. \n"
    },
    {
        "id": 26,
        "term": "Unknown",
        "prompt": "请解释 LDA 是如何对 Document-Topic-Word 分布进行建模的？",
        "response": "LDA假设每个文档由多个主题组成，每个主题由一个概率分布决定了它会包含哪些词语。 \n"
    },
    {
        "id": 27,
        "term": "Unknown",
        "prompt": "SVM 引入 Kernel Trick 主要是为了解决什么类型的数据分类问题？",
        "response": "SVM 引入 Kernel Trick 来解决非线性可分的数据分类问题。 \n"
    },
    {
        "id": 28,
        "term": "Unknown",
        "prompt": "在复杂推理任务中，CoT 提示策略的核心优势是什么？",
        "response": "CoT 提示策略的核心优势在于能够引导大型语言模型进行**few-shot learning**，并通过明确的步骤和示例，提高其在复杂推理任务中的准确性和鲁棒性。 \n"
    },
    {
        "id": 29,
        "term": "Unknown",
        "prompt": "在 IE 流水线中，NER 的准确率对后续 Relation Extraction 有什么影响？",
        "response": "High NER accuracy in the IE pipeline generally improves Relation Extraction performance. \n"
    },
    {
        "id": 30,
        "term": "Unknown",
        "prompt": "将 KG 引入 LLM 推理中，能否有效缓解 Hallucination 问题？",
        "response": "Integrating knowledge graphs (KG) into large language models (LLMs) shows promise in mitigating hallucination by providing factual grounding and reducing reliance on purely statistical predictions. \n"
    },
    {
        "id": 31,
        "term": "Unknown",
        "prompt": "YOLO 系列相比于 Two-stage Detector 在推理速度上的优势源自哪里？",
        "response": "YOLO's speed advantage over Two-stage Detectors stems from its single-stage architecture, which directly predicts bounding boxes and class probabilities in one pass, eliminating the need for region proposal and classification stages. \n"
    },
    {
        "id": 32,
        "term": "Unknown",
        "prompt": "RAG 系统中 Retrieval 阶段的准确性如何影响最终生成的 Fidelity？",
        "response": "The accuracy of the Retrieval stage in a RAG system directly impacts the Fidelity of the generated response, as inaccurate retrieval of relevant context can lead to factually incorrect or irrelevant outputs. \n"
    }
]