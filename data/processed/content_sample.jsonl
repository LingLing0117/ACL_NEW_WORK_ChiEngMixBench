{"id": "00000001", "task": "rating", "text": "content_text 从0开始做一篇Benchmark本篇文章记录从0开始完成一篇Benchmark的经验。", "spans": [{"unit": "char", "start": 0, "end": 7, "lang": "en"}, {"unit": "char", "start": 8, "end": 12, "lang": "en"}, {"unit": "char", "start": 20, "end": 29, "lang": "en"}, {"unit": "char", "start": 43, "end": 52, "lang": "en"}], "emd": 0.5686, "context_domain": "content", "dedup_hash": "sha1:d4107df2ed965867024663efde4a73cf8b782686"}
{"id": "00000002", "task": "rating", "text": "什么是Benchmark?", "spans": [{"unit": "char", "start": 3, "end": 12, "lang": "en"}], "emd": 0.75, "context_domain": "content", "dedup_hash": "sha1:b9e84fc7f29e825107794816167c58334de0457e"}
{"id": "00000003", "task": "rating", "text": "Benchmark是AI发展的重要环节,通常是提出若干个任务,然后对某个领域的一系列模型进行评估。", "spans": [{"unit": "char", "start": 0, "end": 9, "lang": "en"}, {"unit": "char", "start": 10, "end": 12, "lang": "en"}], "emd": 0.2391, "context_domain": "content", "dedup_hash": "sha1:64b9b6339fc2085c75191ffb886cb402edeca4b7"}
{"id": "00000004", "task": "rating", "text": "想要做一篇好的Benchmark,有几个必要的组成部分:1.", "spans": [{"unit": "char", "start": 7, "end": 16, "lang": "en"}], "emd": 0.3462, "context_domain": "content", "dedup_hash": "sha1:82e0bce91114be4814fcd5bf9cfc74e04520c94f"}
{"id": "00000005", "task": "rating", "text": "(Benchmark的主体通常为一个数据集,所以需要大量的人来完成数据收集、审核以及清洗的工作。", "spans": [{"unit": "char", "start": 1, "end": 10, "lang": "en"}], "emd": 0.2045, "context_domain": "content", "dedup_hash": "sha1:e27a1711372979e24c1a1c064e546163b6dff66c"}
{"id": "00000006", "task": "rating", "text": "以上就是一个基本的Benchmark应有的部分,但是仅仅有这些,会显得整个文章的贡献度较少,所以Benchmark通常会从以下两个角度将工作进一步深入:提出解决问题的方法:比如我做一个幻觉的评测,那么我需要额外提出一个可能缓解幻觉的方法。", "spans": [{"unit": "char", "start": 9, "end": 18, "lang": "en"}, {"unit": "char", "start": 48, "end": 57, "lang": "en"}], "emd": 0.1607, "context_domain": "content", "dedup_hash": "sha1:2120c0a855a3484bad3bfdf7c4950cef5ff574f9"}
{"id": "00000007", "task": "rating", "text": "使用LLM进行Self-Instruct,相信很多用GPT-3.5或者4进行self-instruct的同学都会发现,调API合成的数据通常存在多样性较差的问题,那么就需要在构建数据的过程中“添油加醋”,比如提供一个模板,提供更丰富的背景,提高temperature等,让模型生成更丰富的数据。", "spans": [{"unit": "char", "start": 2, "end": 5, "lang": "en"}, {"unit": "char", "start": 7, "end": 11, "lang": "en"}, {"unit": "char", "start": 12, "end": 20, "lang": "en"}, {"unit": "char", "start": 26, "end": 29, "lang": "en"}, {"unit": "char", "start": 38, "end": 42, "lang": "en"}, {"unit": "char", "start": 43, "end": 51, "lang": "en"}, {"unit": "char", "start": 60, "end": 63, "lang": "en"}, {"unit": "char", "start": 122, "end": 133, "lang": "en"}], "emd": 0.3385, "context_domain": "content", "dedup_hash": "sha1:ed01ec0a50c650486bfc8c65147cbbc690846695"}
{"id": "00000008", "task": "rating", "text": "网络爬取 &amp;", "spans": [{"unit": "char", "start": 6, "end": 9, "lang": "en"}], "emd": 0.4286, "context_domain": "content", "dedup_hash": "sha1:a1088d0d0960f5046229fdab2313be2a9d9fb40a"}
{"id": "00000009", "task": "rating", "text": "在构建完数据后,通常需要多人对数据的质量进行review,也就是quality control,这里通常检查数据是否存在质量或者道德上的问题。", "spans": [{"unit": "char", "start": 22, "end": 28, "lang": "en"}, {"unit": "char", "start": 32, "end": 39, "lang": "en"}, {"unit": "char", "start": 40, "end": 47, "lang": "en"}], "emd": 0.303, "context_domain": "content", "dedup_hash": "sha1:17125fddba0feff8838f146e64ade8d7e5735e83"}
{"id": "00000010", "task": "rating", "text": "同时数据,应该以统一的格式进行管理,并且每个数据集包含一个meta.txt来说明这份数据集的数据量、用途以及构建时间。", "spans": [{"unit": "char", "start": 29, "end": 33, "lang": "en"}, {"unit": "char", "start": 34, "end": 37, "lang": "en"}], "emd": 0.1296, "context_domain": "content", "dedup_hash": "sha1:0896df677e6f3937badff603d6caff5b9ebb372f"}
{"id": "00000011", "task": "rating", "text": "同时,众包的过程中应该保持统一的protocol,需要注意的点是:数据命名:不要缺省或者添加额外的0,比如我需要命名为0030,结果直接命名为30。", "spans": [{"unit": "char", "start": 16, "end": 24, "lang": "en"}], "emd": 0.1333, "context_domain": "content", "dedup_hash": "sha1:710219826927a933c5e8de763390177baf69d7bb"}
{"id": "00000012", "task": "rating", "text": "注意冗余文件:不要出现.DStore等冗余的文件,否则批量处理时容易导致index编号错误。", "spans": [{"unit": "char", "start": 12, "end": 18, "lang": "en"}, {"unit": "char", "start": 36, "end": 41, "lang": "en"}], "emd": 0.2619, "context_domain": "content", "dedup_hash": "sha1:f28cfb15f1d15f8953a958e7088ca5126b73e429"}
{"id": "00000013", "task": "rating", "text": "上述问题可以在清洗数据的过程中,通过在代码中引入special case进行处理,但是最好还是在源头就提高质量。", "spans": [{"unit": "char", "start": 24, "end": 31, "lang": "en"}, {"unit": "char", "start": 32, "end": 36, "lang": "en"}], "emd": 0.2115, "context_domain": "content", "dedup_hash": "sha1:6424a79bf0a71d6db3d68fdc62c9ec02fa80e846"}
{"id": "00000014", "task": "rating", "text": "比如True/False,或者选择题,或者进行子串匹配。", "spans": [{"unit": "char", "start": 2, "end": 6, "lang": "en"}, {"unit": "char", "start": 7, "end": 12, "lang": "en"}], "emd": 0.375, "context_domain": "content", "dedup_hash": "sha1:607cabe325188f4eed254224641f8918d0a43cd4"}
{"id": "00000015", "task": "rating", "text": "基于Metric。", "spans": [{"unit": "char", "start": 2, "end": 8, "lang": "en"}], "emd": 0.75, "context_domain": "content", "dedup_hash": "sha1:ca65c12099d3efde6d5bb4d4ddbf640f845fc9b1"}
{"id": "00000016", "task": "rating", "text": "比如BLEU,PPL这些常用的Metric,通常还会对这些Metrics进行组合,提出一个新的Metric。", "spans": [{"unit": "char", "start": 2, "end": 6, "lang": "en"}, {"unit": "char", "start": 7, "end": 10, "lang": "en"}, {"unit": "char", "start": 15, "end": 21, "lang": "en"}, {"unit": "char", "start": 29, "end": 36, "lang": "en"}, {"unit": "char", "start": 47, "end": 53, "lang": "en"}], "emd": 0.52, "context_domain": "content", "dedup_hash": "sha1:cf5af152bf2603f3385313c579c28edbefa728ff"}
{"id": "00000017", "task": "rating", "text": "现有的工作很喜欢用闭源LLM比如GPT进行Zero-Shot和ICL这些闭源模型进行评分。", "spans": [{"unit": "char", "start": 11, "end": 14, "lang": "en"}, {"unit": "char", "start": 16, "end": 19, "lang": "en"}, {"unit": "char", "start": 21, "end": 25, "lang": "en"}, {"unit": "char", "start": 26, "end": 30, "lang": "en"}, {"unit": "char", "start": 31, "end": 34, "lang": "en"}], "emd": 0.3953, "context_domain": "content", "dedup_hash": "sha1:a55a14699959a689cbaa8ba4cf9889802b739644"}
{"id": "00000018", "task": "rating", "text": "但是实际使用中都会发现,使用这些LLM存在一定的问题:潜在的Bias。", "spans": [{"unit": "char", "start": 16, "end": 19, "lang": "en"}, {"unit": "char", "start": 30, "end": 34, "lang": "en"}], "emd": 0.2188, "context_domain": "content", "dedup_hash": "sha1:b913d0ca84447bd616b8e106c950d03bc43c9ea4"}
{"id": "00000019", "task": "rating", "text": "比如有工作研究LLM会识别出自己LLM的回答,并且给出更高的评价。", "spans": [{"unit": "char", "start": 7, "end": 10, "lang": "en"}, {"unit": "char", "start": 16, "end": 19, "lang": "en"}], "emd": 0.1935, "context_domain": "content", "dedup_hash": "sha1:5b096048f3b105b4a061180d3e878b2653854409"}
{"id": "00000020", "task": "rating", "text": "答案通常需要清洗,一般会用Please just answer with number(这样的prompt进行约束)。", "spans": [{"unit": "char", "start": 13, "end": 19, "lang": "en"}, {"unit": "char", "start": 20, "end": 24, "lang": "en"}, {"unit": "char", "start": 25, "end": 31, "lang": "en"}, {"unit": "char", "start": 32, "end": 36, "lang": "en"}, {"unit": "char", "start": 37, "end": 43, "lang": "en"}, {"unit": "char", "start": 47, "end": 53, "lang": "en"}], "emd": 0.6275, "context_domain": "content", "dedup_hash": "sha1:818a0272931e1708f9eda9e10c3ce81773175469"}
{"id": "00000021", "task": "rating", "text": "对于简单的问题,GPT确实能很好的回答,但是这样的话,benchmark中的prompt其实在一定程度上就缺乏价值了,但是对于困难的问题,GPT很难从简短的prompt中理解。", "spans": [{"unit": "char", "start": 8, "end": 11, "lang": "en"}, {"unit": "char", "start": 27, "end": 36, "lang": "en"}, {"unit": "char", "start": 38, "end": 44, "lang": "en"}, {"unit": "char", "start": 69, "end": 72, "lang": "en"}, {"unit": "char", "start": 78, "end": 84, "lang": "en"}], "emd": 0.3293, "context_domain": "content", "dedup_hash": "sha1:3b9f9e1ee1ffd35a66ad1cedc053306cb204bcac"}
{"id": "00000022", "task": "rating", "text": "大家都知道的是,随着时间的推移,GPT的能力是会发生变化的,而且完全受控于OpenAI,这样的Evaluator给打分过程带来了不稳定性。", "spans": [{"unit": "char", "start": 16, "end": 19, "lang": "en"}, {"unit": "char", "start": 37, "end": 43, "lang": "en"}, {"unit": "char", "start": 47, "end": 56, "lang": "en"}], "emd": 0.2812, "context_domain": "content", "dedup_hash": "sha1:59cc21e2d1d8f8602a8a09020cad2fa818bbcdc5"}
{"id": "00000023", "task": "rating", "text": "但是使用Specific-Evaluator也存在一些问题:需要人工标注:数据少了Specific-Evaluator就训不出来,所以需要大量的人工标注数据。", "spans": [{"unit": "char", "start": 4, "end": 12, "lang": "en"}, {"unit": "char", "start": 13, "end": 22, "lang": "en"}, {"unit": "char", "start": 41, "end": 49, "lang": "en"}, {"unit": "char", "start": 50, "end": 59, "lang": "en"}], "emd": 0.4658, "context_domain": "content", "dedup_hash": "sha1:eb15f6a13def3eee401218336d3a900485118f0b"}
{"id": "00000024", "task": "rating", "text": "(Research and Analysis)Research and Analysis通常指的是对实验结果进行分析,并且通过已有或者补充的实验,给出新的实验发现,起到一个提高论文价值、为同领域同行指出新的研究方向的作用。", "spans": [{"unit": "char", "start": 1, "end": 9, "lang": "en"}, {"unit": "char", "start": 10, "end": 13, "lang": "en"}, {"unit": "char", "start": 14, "end": 22, "lang": "en"}, {"unit": "char", "start": 23, "end": 31, "lang": "en"}, {"unit": "char", "start": 32, "end": 35, "lang": "en"}, {"unit": "char", "start": 36, "end": 44, "lang": "en"}], "emd": 0.38, "context_domain": "content", "dedup_hash": "sha1:dbfa35e3033c562c0dd2b2af7dbe32aa2d72ffab"}
{"id": "00000025", "task": "rating", "text": "Research and Analysis是整个Benchmark中最容易出technical insight的部分,也是我认为最考验researcher对这个领域理解程度的部分。", "spans": [{"unit": "char", "start": 0, "end": 8, "lang": "en"}, {"unit": "char", "start": 9, "end": 12, "lang": "en"}, {"unit": "char", "start": 13, "end": 21, "lang": "en"}, {"unit": "char", "start": 24, "end": 33, "lang": "en"}, {"unit": "char", "start": 38, "end": 47, "lang": "en"}, {"unit": "char", "start": 48, "end": 55, "lang": "en"}, {"unit": "char", "start": 67, "end": 77, "lang": "en"}], "emd": 0.6353, "context_domain": "content", "dedup_hash": "sha1:c72632feb38ef7cafda5e7db13dd42a106a9ae9e"}
{"id": "00000026", "task": "rating", "text": "RA如果写的泛泛而谈,则很容易被argue没有novelty或者不够surprising;", "spans": [{"unit": "char", "start": 16, "end": 21, "lang": "en"}, {"unit": "char", "start": 23, "end": 30, "lang": "en"}, {"unit": "char", "start": 34, "end": 44, "lang": "en"}], "emd": 0.5366, "context_domain": "content", "dedup_hash": "sha1:a0666770ac256dd69cb9ace28bbb0ebea3ebf633"}
{"id": "00000027", "task": "rating", "text": "而如果写的过于浮夸,实验验证跟不上,又有可能会被reviewer argue overclaim。", "spans": [{"unit": "char", "start": 24, "end": 32, "lang": "en"}, {"unit": "char", "start": 33, "end": 38, "lang": "en"}, {"unit": "char", "start": 39, "end": 48, "lang": "en"}], "emd": 0.5, "context_domain": "content", "dedup_hash": "sha1:c2b873e80bfbf9abcf485d21084d3d3d31de3858"}
{"id": "00000028", "task": "rating", "text": "有一些很常见的消融思路,比如说Model Size,Direction,training dataset等。", "spans": [{"unit": "char", "start": 15, "end": 20, "lang": "en"}, {"unit": "char", "start": 21, "end": 25, "lang": "en"}, {"unit": "char", "start": 26, "end": 35, "lang": "en"}, {"unit": "char", "start": 36, "end": 44, "lang": "en"}, {"unit": "char", "start": 45, "end": 52, "lang": "en"}], "emd": 0.6875, "context_domain": "content", "dedup_hash": "sha1:f97634be73645377f68798113d55c7db8e1847d3"}
{"id": "00000029", "task": "rating", "text": "如何写Related Work?", "spans": [{"unit": "char", "start": 3, "end": 10, "lang": "en"}, {"unit": "char", "start": 11, "end": 15, "lang": "en"}], "emd": 0.7857, "context_domain": "content", "dedup_hash": "sha1:bc4c65f2d379918e2ca954e12cca605291d128e2"}
{"id": "00000030", "task": "rating", "text": "Related Work通常为2~3个小的段落,其作用为给出与领域相关的工作,以便读者能够通过你的这篇论文找到同领域的其他论文。", "spans": [{"unit": "char", "start": 0, "end": 7, "lang": "en"}, {"unit": "char", "start": 8, "end": 12, "lang": "en"}], "emd": 0.193, "context_domain": "content", "dedup_hash": "sha1:244922f4f56cf0083a66196755270207787ef2ec"}
{"id": "00000031", "task": "rating", "text": "在写Related Work时,我们常常会像写中文论文的关键词一样,以关键词为种子,衍生到一个具体的Topic。", "spans": [{"unit": "char", "start": 2, "end": 9, "lang": "en"}, {"unit": "char", "start": 10, "end": 14, "lang": "en"}, {"unit": "char", "start": 50, "end": 55, "lang": "en"}], "emd": 0.3137, "context_domain": "content", "dedup_hash": "sha1:b1df93437d99bef1fd2c1f516d1b114a31af925b"}
{"id": "00000032", "task": "rating", "text": "比如说我做一个大模型幻觉的评测,那么我的关键词也许是大语言模型,有些人会直接对Large Language Models做Related work,那么我认为这样的作法是对这个部分有点不负责的,更好的做法可能是像DFS一样把depth设置为2,即再往下深入去寻找一层,直接以Benchmarks for LLM Hallucination会更好一点。", "spans": [{"unit": "char", "start": 39, "end": 44, "lang": "en"}, {"unit": "char", "start": 45, "end": 53, "lang": "en"}, {"unit": "char", "start": 54, "end": 60, "lang": "en"}, {"unit": "char", "start": 61, "end": 68, "lang": "en"}, {"unit": "char", "start": 69, "end": 73, "lang": "en"}, {"unit": "char", "start": 106, "end": 109, "lang": "en"}, {"unit": "char", "start": 112, "end": 117, "lang": "en"}, {"unit": "char", "start": 137, "end": 147, "lang": "en"}, {"unit": "char", "start": 148, "end": 151, "lang": "en"}, {"unit": "char", "start": 152, "end": 155, "lang": "en"}, {"unit": "char", "start": 156, "end": 169, "lang": "en"}], "emd": 0.4161, "context_domain": "content", "dedup_hash": "sha1:5f45b666f4dd72c2d806b9ced015cc1b442586af"}
{"id": "00000033", "task": "rating", "text": "当选定Topic后,就要寻找这个Topic的论文路线,随后就是经典的聚类。", "spans": [{"unit": "char", "start": 3, "end": 8, "lang": "en"}, {"unit": "char", "start": 16, "end": 21, "lang": "en"}], "emd": 0.2941, "context_domain": "content", "dedup_hash": "sha1:e495669a6726d6f4b48b598aa30f1b58e7a5968c"}
{"id": "00000034", "task": "rating", "text": "比如现在对LLM Hallucination的Benchmark路线通常分为几种,谁谁谁属于第一种,谁谁谁属于第二种。", "spans": [{"unit": "char", "start": 5, "end": 8, "lang": "en"}, {"unit": "char", "start": 9, "end": 22, "lang": "en"}, {"unit": "char", "start": 23, "end": 32, "lang": "en"}], "emd": 0.4545, "context_domain": "content", "dedup_hash": "sha1:788612f0d805d46ca696fa5ae5fc52d831ddf58d"}
{"id": "00000035", "task": "rating", "text": "首先进入Deepseek官网主页我们看到的是这样的:可以看到,榜单左侧Benchmark这一列列举了很多种不知是啥的简称,英文、编程、数学和中文对应的种类也各不相同。", "spans": [{"unit": "char", "start": 4, "end": 12, "lang": "en"}, {"unit": "char", "start": 35, "end": 44, "lang": "en"}], "emd": 0.2208, "context_domain": "content", "dedup_hash": "sha1:f5b3d8ea3ecc246621a55ff2b468be6b4033665b"}
{"id": "00000036", "task": "rating", "text": "Benchmark到底是什么,这篇文章将试图介绍一下。", "spans": [{"unit": "char", "start": 0, "end": 9, "lang": "en"}], "emd": 0.36, "context_domain": "content", "dedup_hash": "sha1:6e399517c09b107da320c35f827a975a7592284f"}
{"id": "00000037", "task": "rating", "text": "这些Benchmark为大模型的开发者和使用者提供了一个统一的评估框架,帮助研究人员和开发者衡量模型在不同领域(如语言理解、生成、推理等)的能力,推动技术进步,同时也为用户在选择适合特定任务的大模型时提供参考依据。", "spans": [{"unit": "char", "start": 2, "end": 11, "lang": "en"}], "emd": 0.0909, "context_domain": "content", "dedup_hash": "sha1:bf7f1b1c529000004c8effeb234919bec7843032"}
{"id": "00000038", "task": "rating", "text": "但是大家容易发现一个问题,一个领域的Benchmark这么多,究竟哪一个更可靠?", "spans": [{"unit": "char", "start": 18, "end": 27, "lang": "en"}], "emd": 0.2432, "context_domain": "content", "dedup_hash": "sha1:61c60bdd5f6fad8d3e5e25beede036e87b4c76c4"}
{"id": "00000039", "task": "rating", "text": "这就会涉及到Benchmark评价的问题了。", "spans": [{"unit": "char", "start": 6, "end": 15, "lang": "en"}], "emd": 0.4286, "context_domain": "content", "dedup_hash": "sha1:8dbf7a0a52637af86cc2e8cc4cbad38dd497e53d"}
{"id": "00000040", "task": "rating", "text": "所以,模型可能在特定Benchmark上过拟合(针对该Benchmark的特定调优),导致实际应用表现不佳。", "spans": [{"unit": "char", "start": 10, "end": 19, "lang": "en"}, {"unit": "char", "start": 27, "end": 36, "lang": "en"}], "emd": 0.3673, "context_domain": "content", "dedup_hash": "sha1:7e95bc88fbc4a8e66255b6f78759fd4a56f31d19"}
{"id": "00000041", "task": "rating", "text": "某些Benchmark仅关注性能,忽略效率、能耗或伦理问题。", "spans": [{"unit": "char", "start": 2, "end": 11, "lang": "en"}], "emd": 0.3333, "context_domain": "content", "dedup_hash": "sha1:8f8a0018482db40e10dd1f661b10184e003bd83f"}
{"id": "00000042", "task": "rating", "text": "那最终某一个Benchmark好坏,用户体验应该是重要的一环。", "spans": [{"unit": "char", "start": 6, "end": 15, "lang": "en"}], "emd": 0.3103, "context_domain": "content", "dedup_hash": "sha1:caa0604f16f00238bd4253292dba2bb7820e4db3"}
{"id": "00000043", "task": "rating", "text": "合成数据,通过规则或生成模型(如GPT)创建数据,但需验证真实性。", "spans": [{"unit": "char", "start": 16, "end": 19, "lang": "en"}], "emd": 0.1071, "context_domain": "content", "dedup_hash": "sha1:9ff7f739cb00db44dc2d32827d75d50a827789be"}
{"id": "00000044", "task": "rating", "text": "任务设计与评价指标这一部分是在评分榜单中经常能看到的,比如模型结果是根据PASS@1‌排名的。", "spans": [{"unit": "char", "start": 36, "end": 40, "lang": "en"}], "emd": 0.0952, "context_domain": "content", "dedup_hash": "sha1:e7eda5ebbc65357dca9ea26d1f65c867a43e8756"}
{"id": "00000045", "task": "rating", "text": "PASS@1是评估模型性能的一个指标,特别是在自然语言处理(NLP)和代码生成任务中常用。", "spans": [{"unit": "char", "start": 0, "end": 4, "lang": "en"}, {"unit": "char", "start": 30, "end": 33, "lang": "en"}], "emd": 0.1795, "context_domain": "content", "dedup_hash": "sha1:10e204d7e8c66f07600dcc1e0ff07bdc6105c99c"}
{"id": "00000046", "task": "rating", "text": "具体来说,‌PASS@1‌衡量的是模型在给定上下文后,第一次尝试就正确生成或选择正确答案的比例。", "spans": [{"unit": "char", "start": 6, "end": 10, "lang": "en"}], "emd": 0.0976, "context_domain": "content", "dedup_hash": "sha1:b5d23ce9b8477e4b1c8ff0e050db754e4694299a"}
{"id": "00000047", "task": "rating", "text": "迭代与验证小规模试点:先在小数据集上测试Benchmark的合理性。", "spans": [{"unit": "char", "start": 20, "end": 29, "lang": "en"}], "emd": 0.2812, "context_domain": "content", "dedup_hash": "sha1:e9310129199d866d899cf25a6346e9c895a7662b"}
{"id": "00000048", "task": "rating", "text": "三、Benchmark的评估指标大模型的Benchmark 评价指标用于量化模型在不同任务中的性能表现,以下是详细的分类和解释:通用语言任务指标1.", "spans": [{"unit": "char", "start": 2, "end": 11, "lang": "en"}, {"unit": "char", "start": 20, "end": 29, "lang": "en"}], "emd": 0.2647, "context_domain": "content", "dedup_hash": "sha1:13db5e44eb9a9e98d50be45760cc132a3f762f34"}
{"id": "00000049", "task": "rating", "text": "常见变体包括ROUGE-N:n-gram重叠率;", "spans": [{"unit": "char", "start": 6, "end": 11, "lang": "en"}, {"unit": "char", "start": 16, "end": 20, "lang": "en"}], "emd": 0.5, "context_domain": "content", "dedup_hash": "sha1:4b61b11f8a3732d6e15657063e39b8cb5444a0b1"}
{"id": "00000050", "task": "rating", "text": "ROUGE-L:基于最长公共子序列的相似度。", "spans": [{"unit": "char", "start": 0, "end": 5, "lang": "en"}], "emd": 0.2778, "context_domain": "content", "dedup_hash": "sha1:8737b4faac62fdb87cafe633e36b120efef46bf6"}
{"id": "00000051", "task": "rating", "text": "优点是可以捕捉语义一致性,优于n-gram方法。", "spans": [{"unit": "char", "start": 17, "end": 21, "lang": "en"}], "emd": 0.2, "context_domain": "content", "dedup_hash": "sha1:8e1dcd1bcbd452e76a56a7715fd426d31e14c856"}
{"id": "00000052", "task": "rating", "text": "缺点是无法直接反映生成质量(如流畅但无意义的文本可能PPL低)。", "spans": [{"unit": "char", "start": 26, "end": 29, "lang": "en"}], "emd": 0.1034, "context_domain": "content", "dedup_hash": "sha1:eea1313675162ecff0d643d8ab0dd2ceebd05326"}
{"id": "00000053", "task": "rating", "text": "Pass@k:生成k个代码样本中至少有一个通过单元测试的比例。", "spans": [{"unit": "char", "start": 0, "end": 4, "lang": "en"}], "emd": 0.1538, "context_domain": "content", "dedup_hash": "sha1:842ea048e8580e084c863a29165638f592e639ba"}
{"id": "00000054", "task": "rating", "text": "适用代码生成(如HumanEval)任务。", "spans": [{"unit": "char", "start": 8, "end": 17, "lang": "en"}], "emd": 0.5, "context_domain": "content", "dedup_hash": "sha1:4038d60dda997f2d2128322b5c7ab27a24dcc1a7"}
{"id": "00000055", "task": "rating", "text": "常见的Pass@1表示首次生成即正确的概率。", "spans": [{"unit": "char", "start": 3, "end": 7, "lang": "en"}], "emd": 0.2105, "context_domain": "content", "dedup_hash": "sha1:059f78c675552fdfc3928867c6b6311006dad3c1"}
{"id": "00000056", "task": "rating", "text": "适用数学推理(如GSM8K、MATH)等任务。", "spans": [{"unit": "char", "start": 14, "end": 18, "lang": "en"}], "emd": 0.2857, "context_domain": "content", "dedup_hash": "sha1:e6255e1f8dc771a28d12a3fb59d84d6c07ba9888"}
{"id": "00000057", "task": "rating", "text": "适用视觉问答(VQA)、物体检测等任务。", "spans": [{"unit": "char", "start": 7, "end": 10, "lang": "en"}], "emd": 0.1875, "context_domain": "content", "dedup_hash": "sha1:4c2bf1d9207b1d36f3f4c8f75b1fb99794cb0c06"}
{"id": "00000058", "task": "rating", "text": "GLUE Benchmark,SuperGLUE Benchmark通用语言理解评估(GLUE)基准是一套用于训练、评估和分析自然语言理解系统的资源集合。", "spans": [{"unit": "char", "start": 0, "end": 4, "lang": "en"}, {"unit": "char", "start": 5, "end": 14, "lang": "en"}, {"unit": "char", "start": 15, "end": 24, "lang": "en"}, {"unit": "char", "start": 25, "end": 34, "lang": "en"}, {"unit": "char", "start": 43, "end": 47, "lang": "en"}], "emd": 0.5, "context_domain": "content", "dedup_hash": "sha1:bc3c0f69ebba68f12cadac010aa1a752c53ab3b0"}
{"id": "00000059", "task": "rating", "text": "GLUE包含以下组成部分:一个由九项句子或句子对语言理解任务构成的基准测试集,这些任务基于现有成熟数据集构建,其选择涵盖了不同数据规模、文本类型和难度等级;", "spans": [{"unit": "char", "start": 0, "end": 4, "lang": "en"}], "emd": 0.0548, "context_domain": "content", "dedup_hash": "sha1:28ffb50a374262be5fdc54e3399d959f67d05be6"}
{"id": "00000060", "task": "rating", "text": "GLUE基准采用与模型无关的架构设计,任何能够处理句子/句子对并生成相应预测的系统均可参与。", "spans": [{"unit": "char", "start": 0, "end": 4, "lang": "en"}], "emd": 0.093, "context_domain": "content", "dedup_hash": "sha1:dff24985e1c10362355df44450232e204c57d977"}
{"id": "00000061", "task": "rating", "text": "GLUE的最终目标是推动通用且鲁棒的自然语言理解系统的研究发展。", "spans": [{"unit": "char", "start": 0, "end": 4, "lang": "en"}], "emd": 0.129, "context_domain": "content", "dedup_hash": "sha1:aa2812718a4fc37e7d5dccede11079ab09d29472"}
{"id": "00000062", "task": "rating", "text": "于晨晨:GLUE基准数据集介绍及下载2.", "spans": [{"unit": "char", "start": 4, "end": 8, "lang": "en"}], "emd": 0.2353, "context_domain": "content", "dedup_hash": "sha1:668a059f5eaa20059c33b6a6c8973f54b9fba0da"}
{"id": "00000063", "task": "rating", "text": "Human-EvalHumanEval是文章中提到的一个手写代码评测集,专门用于评估训练在编程任务上的大规模语言模型的能力。", "spans": [{"unit": "char", "start": 0, "end": 5, "lang": "en"}, {"unit": "char", "start": 6, "end": 19, "lang": "en"}], "emd": 0.3051, "context_domain": "content", "dedup_hash": "sha1:f78d4b7a8131427e35e911621f40919bc44bd59d"}
{"id": "00000064", "task": "rating", "text": "HumanEval 的特点:代码难度:题目从简单的排序、查找问题到较为复杂的字符串处理、递归问题不等,确保评测集能够考察到模型在处理不同复杂度的代码生成任务时的表现。", "spans": [{"unit": "char", "start": 0, "end": 9, "lang": "en"}], "emd": 0.1184, "context_domain": "content", "dedup_hash": "sha1:d9e8fa5a497eb6b09c05874bac0a89e14edabfd1"}
{"id": "00000065", "task": "rating", "text": "测试模型coding能力的评测集HumanEval评估原理及指标解析:分析源码-CSDN博客4.", "spans": [{"unit": "char", "start": 4, "end": 10, "lang": "en"}, {"unit": "char", "start": 16, "end": 25, "lang": "en"}, {"unit": "char", "start": 40, "end": 44, "lang": "en"}], "emd": 0.4318, "context_domain": "content", "dedup_hash": "sha1:6d6c74a5cc9d8245d4a518741fe8846867072c78"}
{"id": "00000066", "task": "rating", "text": "该基准涵盖57个学科领域,横跨科学、技术、工程、数学(STEM)、人文学科、社会科学等多个维度,难度梯度从基础教育阶段延伸至专业高级水平,同时考察世界知识储备与问题解决能力。", "spans": [{"unit": "char", "start": 27, "end": 31, "lang": "en"}], "emd": 0.0541, "context_domain": "content", "dedup_hash": "sha1:234c6ffea87d4b58e2835d8f0fa710c085db9cbc"}
{"id": "00000067", "task": "rating", "text": "具体到某一个Benchmark下,一般也会有对应的模型排名。", "spans": [{"unit": "char", "start": 6, "end": 15, "lang": "en"}], "emd": 0.3214, "context_domain": "content", "dedup_hash": "sha1:05e70b98581fa0066f1ab961a9fd104ea06e9a99"}
{"id": "00000068", "task": "rating", "text": "以及非常值得推荐的其他知友的总结:tomsheep:全景解读 LLM Agent 评测方法 现在的科研环境对于风险的容忍度很低,同时对于确定性的渴望极大。", "spans": [{"unit": "char", "start": 17, "end": 25, "lang": "en"}, {"unit": "char", "start": 31, "end": 34, "lang": "en"}, {"unit": "char", "start": 35, "end": 40, "lang": "en"}], "emd": 0.2319, "context_domain": "content", "dedup_hash": "sha1:11eb866e3652407b41c8dcee352b3bc353da8e6f"}
{"id": "00000069", "task": "rating", "text": "benchmark文章风险低,确定性大,抢的早还有一定impact。", "spans": [{"unit": "char", "start": 0, "end": 9, "lang": "en"}, {"unit": "char", "start": 27, "end": 33, "lang": "en"}], "emd": 0.4839, "context_domain": "content", "dedup_hash": "sha1:09876b8cf9381d454c74b639d7a8c713fcbcab8d"}
{"id": "00000070", "task": "rating", "text": "同理一堆搞开源repo的也类似。", "spans": [{"unit": "char", "start": 7, "end": 11, "lang": "en"}], "emd": 0.2667, "context_domain": "content", "dedup_hash": "sha1:6ab0c4f91e958dcadcff814f3f4da26bfb011c92"}
{"id": "00000071", "task": "rating", "text": "SOTA全称是state of the art,是指在特定任务中目前表现最好的方法或模型。", "spans": [{"unit": "char", "start": 0, "end": 4, "lang": "en"}, {"unit": "char", "start": 7, "end": 12, "lang": "en"}, {"unit": "char", "start": 16, "end": 19, "lang": "en"}, {"unit": "char", "start": 20, "end": 23, "lang": "en"}], "emd": 0.3947, "context_domain": "content", "dedup_hash": "sha1:8ac2267e05a25e459639afb4b81c6b1eba2ef312"}
{"id": "00000072", "task": "rating", "text": "Benchmark和baseline都是指最基础的比较对象。", "spans": [{"unit": "char", "start": 0, "end": 9, "lang": "en"}, {"unit": "char", "start": 10, "end": 18, "lang": "en"}], "emd": 0.5862, "context_domain": "content", "dedup_hash": "sha1:e8c418ccf8cc947a9ceb50ba9ce9a80ef3eb24b8"}
{"id": "00000073", "task": "rating", "text": "你论文的motivation来自于想超越现有的baseline/benchmark,你的实验数据都需要以baseline/benckmark为基准来判断是否有提高。", "spans": [{"unit": "char", "start": 4, "end": 14, "lang": "en"}, {"unit": "char", "start": 23, "end": 31, "lang": "en"}, {"unit": "char", "start": 32, "end": 41, "lang": "en"}, {"unit": "char", "start": 52, "end": 60, "lang": "en"}, {"unit": "char", "start": 61, "end": 70, "lang": "en"}], "emd": 0.5641, "context_domain": "content", "dedup_hash": "sha1:7384b773d0ced54dcf3624062f3f46144a914665"}
{"id": "00000074", "task": "rating", "text": "唯一的区别就是baseline讲究一套方法,而benchmark更偏向于一个目前最高的指标,比如precision,recall等等可量化的指标。", "spans": [{"unit": "char", "start": 7, "end": 15, "lang": "en"}, {"unit": "char", "start": 23, "end": 32, "lang": "en"}, {"unit": "char", "start": 48, "end": 57, "lang": "en"}, {"unit": "char", "start": 58, "end": 64, "lang": "en"}], "emd": 0.4638, "context_domain": "content", "dedup_hash": "sha1:fefad2d4081daf58b5150627535a01dc392b13e8"}
{"id": "00000075", "task": "rating", "text": "举个例子,NLP任务中BERT是目前的SOTA,你有idea可以超过BERT。", "spans": [{"unit": "char", "start": 5, "end": 8, "lang": "en"}, {"unit": "char", "start": 11, "end": 15, "lang": "en"}, {"unit": "char", "start": 19, "end": 23, "lang": "en"}, {"unit": "char", "start": 26, "end": 30, "lang": "en"}, {"unit": "char", "start": 34, "end": 38, "lang": "en"}], "emd": 0.5278, "context_domain": "content", "dedup_hash": "sha1:c0e5b85c9818bd71a6fb567556a1919beffe0b7c"}
{"id": "00000076", "task": "rating", "text": "那在论文中的实验部分你的方法需要比较的baseline就是BERT,而需要比较的benchmark就是BERT具体的各项指标。", "spans": [{"unit": "char", "start": 19, "end": 27, "lang": "en"}, {"unit": "char", "start": 29, "end": 33, "lang": "en"}, {"unit": "char", "start": 40, "end": 49, "lang": "en"}, {"unit": "char", "start": 51, "end": 55, "lang": "en"}], "emd": 0.4098, "context_domain": "content", "dedup_hash": "sha1:5ec00113236f0c254d2a8aa28d90b3061b98b239"}
{"id": "00000077", "task": "rating", "text": "最近在做个benchmark的工作(另一个工作的数据部分感觉单独拿出来可以试下ACL),其实我觉得还挺水的;", "spans": [{"unit": "char", "start": 5, "end": 14, "lang": "en"}, {"unit": "char", "start": 39, "end": 42, "lang": "en"}], "emd": 0.24, "context_domain": "content", "dedup_hash": "sha1:210479586a6f75896294c9c445f6ad6694f1aa0f"}
{"id": "00000078", "task": "rating", "text": "然后上ACL过去的accepted papers看了下想看看怎么写,看到一堆角色扮演,调戏llm的真感觉瞎了眼第一次发现论文还能这样写,忽然感觉自己做的起码比起来还有点实用价值 谁实验谁拿一作,脏活累活才算付出,吹逼不算我们组很极端,有篇benchmark,打标的六个本科生都是共一 不是教授强答一波。", "spans": [{"unit": "char", "start": 3, "end": 6, "lang": "en"}, {"unit": "char", "start": 9, "end": 17, "lang": "en"}, {"unit": "char", "start": 18, "end": 24, "lang": "en"}, {"unit": "char", "start": 45, "end": 48, "lang": "en"}, {"unit": "char", "start": 119, "end": 128, "lang": "en"}], "emd": 0.2071, "context_domain": "content", "dedup_hash": "sha1:71bbb10bc718c6525a360c2177408e351c390f70"}
{"id": "00000079", "task": "rating", "text": "这不就是做个项目吗,题我给你了,代码框架github也给了,卡也给了,我说你们先在三个公开的benchmark上面把结果跑对。", "spans": [{"unit": "char", "start": 20, "end": 26, "lang": "en"}, {"unit": "char", "start": 46, "end": 55, "lang": "en"}], "emd": 0.2586, "context_domain": "content", "dedup_hash": "sha1:f44f1fd0a1108d9d81aec2aab9f864676c805c44"}
{"id": "00000080", "task": "rating", "text": "这就是benchmark,大大降低了大家互相对比证明自己NB的沟通成本。", "spans": [{"unit": "char", "start": 3, "end": 12, "lang": "en"}], "emd": 0.2812, "context_domain": "content", "dedup_hash": "sha1:3cab24c970dfdee92a41a2b97228165caccc25f3"}
{"id": "00000081", "task": "rating", "text": "但是,于此同时也容易造成行业内自high,而忽视真正有意义的目标。", "spans": [{"unit": "char", "start": 16, "end": 20, "lang": "en"}], "emd": 0.1333, "context_domain": "content", "dedup_hash": "sha1:1a3632ccbf17ad6c1c958940a2617e8f581c22b1"}
{"id": "00000082", "task": "rating", "text": "做了这么多年的性能, 写了好多性能测试的负载, 我都把他们叫做benchmark.", "spans": [{"unit": "char", "start": 31, "end": 40, "lang": "en"}], "emd": 0.25, "context_domain": "content", "dedup_hash": "sha1:f8d76534c81954cbf3d5e54b0292a77fc226117a"}
{"id": "00000083", "task": "rating", "text": "但是, 在一次技术交流时, 有人突然问我什么是好的benchmark.", "spans": [{"unit": "char", "start": 25, "end": 34, "lang": "en"}], "emd": 0.3, "context_domain": "content", "dedup_hash": "sha1:01b7059f435519e906673d8c0e7dc5e006402f0f"}
{"id": "00000084", "task": "rating", "text": "我懵了, 我习惯直接动手解决问题, 而不是去寻找理论依据.什么是好的benchmark?", "spans": [{"unit": "char", "start": 34, "end": 43, "lang": "en"}], "emd": 0.2368, "context_domain": "content", "dedup_hash": "sha1:a40de370c96f43d295d2936a28119cf8d905d032"}
{"id": "00000085", "task": "rating", "text": "我没想到这么朴素的东西也有人总结了方法论.什么是Benchmark:基准测试定义为“用于竞争性评估和比较系统或组件的标准工具,依据特定特性如性能、可靠性或安全性进行评估”。", "spans": [{"unit": "char", "start": 24, "end": 33, "lang": "en"}], "emd": 0.1139, "context_domain": "content", "dedup_hash": "sha1:97f8b8667fc6fd1e9275f1fea387113077d29e44"}
{"id": "00000086", "task": "rating", "text": "而如果用于非竞争性系统评估和比较的工具称为评分工具(rating tools)。", "spans": [{"unit": "char", "start": 26, "end": 32, "lang": "en"}, {"unit": "char", "start": 33, "end": 38, "lang": "en"}], "emd": 0.3056, "context_domain": "content", "dedup_hash": "sha1:d567dac34922cca80cea8c35fc83b0365db48d2e"}
{"id": "00000087", "task": "rating", "text": "比如mlperf, 我的团队就写了onnx-mlir的实现.什么是好的benchmark以下是基准测试的五个重要标准的详细解释:1.", "spans": [{"unit": "char", "start": 2, "end": 8, "lang": "en"}, {"unit": "char", "start": 17, "end": 21, "lang": "en"}, {"unit": "char", "start": 22, "end": 26, "lang": "en"}, {"unit": "char", "start": 35, "end": 44, "lang": "en"}], "emd": 0.3898, "context_domain": "content", "dedup_hash": "sha1:aad70498a8312ca2adc274ec8ee353b8e61e1672"}
{"id": "00000088", "task": "rating", "text": "这个时候, 有个小技巧, 运行benchmark要有warmup, 跑一段时间, 缓存和动态编译带来的性能抖动问题,会随着时间趋向稳定.3.", "spans": [{"unit": "char", "start": 15, "end": 24, "lang": "en"}, {"unit": "char", "start": 26, "end": 32, "lang": "en"}], "emd": 0.2586, "context_domain": "content", "dedup_hash": "sha1:c2d5f7d17c74ccc58688d36f6707060116935b23"}
{"id": "00000089", "task": "rating", "text": "下次再有人问我这个问题, 我就知道怎么回答了:相关性, 可重复性,公平性, 可验证性,易用性 这段时间在关心一些 benchmarking 的工作,虽然我并不主要甚至从未做 Benchmark,但是好的 benchmark 的影响力无疑是巨大的,今天先挑出 SWE-Bench 原文学习一下。", "spans": [{"unit": "char", "start": 57, "end": 69, "lang": "en"}, {"unit": "char", "start": 87, "end": 96, "lang": "en"}, {"unit": "char", "start": 102, "end": 111, "lang": "en"}, {"unit": "char", "start": 129, "end": 132, "lang": "en"}, {"unit": "char", "start": 133, "end": 138, "lang": "en"}], "emd": 0.3065, "context_domain": "content", "dedup_hash": "sha1:6eeaf5ad3dc1771622a16902238d643f6133f27a"}
{"id": "00000090", "task": "rating", "text": "在这里,记录一些我对什么是好的 benchmark 的思索,主要是基于我对 code-force,SWE-Bench 以及 Chatbot Arena 的理解。", "spans": [{"unit": "char", "start": 16, "end": 25, "lang": "en"}, {"unit": "char", "start": 38, "end": 42, "lang": "en"}, {"unit": "char", "start": 43, "end": 48, "lang": "en"}, {"unit": "char", "start": 49, "end": 52, "lang": "en"}, {"unit": "char", "start": 53, "end": 58, "lang": "en"}, {"unit": "char", "start": 62, "end": 69, "lang": "en"}, {"unit": "char", "start": 70, "end": 75, "lang": "en"}], "emd": 0.5672, "context_domain": "content", "dedup_hash": "sha1:8b6acd9a80f42622d31c374769efe1a0b3464f67"}
{"id": "00000091", "task": "rating", "text": "Abstraction开门见山,提出做了一个什么样的 benchmark。", "spans": [{"unit": "char", "start": 0, "end": 11, "lang": "en"}, {"unit": "char", "start": 27, "end": 36, "lang": "en"}], "emd": 0.5882, "context_domain": "content", "dedup_hash": "sha1:60cb4951ad4e2ef9102f4c2da6e364dd0331b571"}
{"id": "00000092", "task": "rating", "text": "我比较惊讶的是,原文的 abstraction 的前 1/3 让我感觉完全可以精简,一开始说 benchmark LLM 难但是重要,第二句话接着说 real-world software engineering is a rich, sustainable and challenging testbed。", "spans": [{"unit": "char", "start": 12, "end": 23, "lang": "en"}, {"unit": "char", "start": 47, "end": 56, "lang": "en"}, {"unit": "char", "start": 57, "end": 60, "lang": "en"}, {"unit": "char", "start": 75, "end": 79, "lang": "en"}, {"unit": "char", "start": 80, "end": 85, "lang": "en"}, {"unit": "char", "start": 86, "end": 94, "lang": "en"}, {"unit": "char", "start": 95, "end": 106, "lang": "en"}, {"unit": "char", "start": 112, "end": 116, "lang": "en"}, {"unit": "char", "start": 118, "end": 129, "lang": "en"}, {"unit": "char", "start": 130, "end": 133, "lang": "en"}, {"unit": "char", "start": 134, "end": 145, "lang": "en"}, {"unit": "char", "start": 146, "end": 153, "lang": "en"}], "emd": 0.696, "context_domain": "content", "dedup_hash": "sha1:5fbedea4a0cb9efd4b871bd4761a809a0c81e3f9"}
{"id": "00000093", "task": "rating", "text": "第二句话现在来看是 trivial 的,但是回退到 23 年文章发表的时候,想法还是非常新颖。", "spans": [{"unit": "char", "start": 10, "end": 17, "lang": "en"}], "emd": 0.1842, "context_domain": "content", "dedup_hash": "sha1:047562ab69a5f808a4ba10981fd7fdf7a44abb3a"}
{"id": "00000094", "task": "rating", "text": "但是第一句话感觉挺 trivial 的,放在 abstract 这种寸土寸金的地方让我感觉像一段废话...接着,作者介绍了 benchmark 的主要内容,从 12 个 popular python 库中选出了 2294 个 Github SWE issue 和 PR 构造了此 benchmark。", "spans": [{"unit": "char", "start": 10, "end": 17, "lang": "en"}, {"unit": "char", "start": 23, "end": 31, "lang": "en"}, {"unit": "char", "start": 62, "end": 71, "lang": "en"}, {"unit": "char", "start": 85, "end": 92, "lang": "en"}, {"unit": "char", "start": 93, "end": 99, "lang": "en"}, {"unit": "char", "start": 113, "end": 119, "lang": "en"}, {"unit": "char", "start": 120, "end": 123, "lang": "en"}, {"unit": "char", "start": 124, "end": 129, "lang": "en"}, {"unit": "char", "start": 140, "end": 149, "lang": "en"}], "emd": 0.5217, "context_domain": "content", "dedup_hash": "sha1:5123a83eed58e1bce10f246dab03114001968c9f"}
{"id": "00000095", "task": "rating", "text": "而后,经过测试,SOTA models (claude 2 等等)和他们自己 finetune 的 SWE-Llama 在当时只能解决 1.96% 的问题,是非常有挑战性的 benchmark。", "spans": [{"unit": "char", "start": 8, "end": 12, "lang": "en"}, {"unit": "char", "start": 13, "end": 19, "lang": "en"}, {"unit": "char", "start": 21, "end": 27, "lang": "en"}, {"unit": "char", "start": 39, "end": 47, "lang": "en"}, {"unit": "char", "start": 50, "end": 53, "lang": "en"}, {"unit": "char", "start": 54, "end": 59, "lang": "en"}, {"unit": "char", "start": 87, "end": 96, "lang": "en"}], "emd": 0.5616, "context_domain": "content", "dedup_hash": "sha1:e24d23761bfa18dc4b8b100ed686d1d502362e5f"}
{"id": "00000096", "task": "rating", "text": "我个人的感觉,Abs 里面体现出 SWE Bench 的优点主要是:贵在真实:毫无疑问,在给定的 framework 上 fix issue 是比起从头写一个 framework 更加真实的需求,毕竟大多时候都是接过前人的 xx,然后在上面继续开发。", "spans": [{"unit": "char", "start": 7, "end": 10, "lang": "en"}, {"unit": "char", "start": 17, "end": 20, "lang": "en"}, {"unit": "char", "start": 21, "end": 26, "lang": "en"}, {"unit": "char", "start": 49, "end": 58, "lang": "en"}, {"unit": "char", "start": 61, "end": 64, "lang": "en"}, {"unit": "char", "start": 65, "end": 70, "lang": "en"}, {"unit": "char", "start": 80, "end": 89, "lang": "en"}], "emd": 0.3558, "context_domain": "content", "dedup_hash": "sha1:89fadc40766cde1ec8d3a6127370cfc0f1685f20"}
{"id": "00000097", "task": "rating", "text": "而 fix issue 的需求就可以构造 SWE bench,从头到尾写 framework 可能不是很准确,应该叫做“在依赖非常少的情况下开发一个完整的系统”,比如做 leetcode 周赛和测试 code force。", "spans": [{"unit": "char", "start": 2, "end": 5, "lang": "en"}, {"unit": "char", "start": 6, "end": 11, "lang": "en"}, {"unit": "char", "start": 21, "end": 24, "lang": "en"}, {"unit": "char", "start": 25, "end": 30, "lang": "en"}, {"unit": "char", "start": 37, "end": 46, "lang": "en"}, {"unit": "char", "start": 85, "end": 93, "lang": "en"}, {"unit": "char", "start": 100, "end": 104, "lang": "en"}, {"unit": "char", "start": 105, "end": 110, "lang": "en"}], "emd": 0.4468, "context_domain": "content", "dedup_hash": "sha1:98b5063de44d9b8ba5698bab21fed4bff3458c42"}
{"id": "00000098", "task": "rating", "text": "难度巨大:以我自己的切身体会,让从 23 年开始算两年后的 LLM 来和我一起写 SGLang 的代码都仍旧糟糕,可想而知在当时 SWE-Bench 的难度有多么巨大。", "spans": [{"unit": "char", "start": 30, "end": 33, "lang": "en"}, {"unit": "char", "start": 41, "end": 47, "lang": "en"}, {"unit": "char", "start": 65, "end": 68, "lang": "en"}, {"unit": "char", "start": 69, "end": 74, "lang": "en"}], "emd": 0.2464, "context_domain": "content", "dedup_hash": "sha1:0cdba830bea2479ed3c51db74b2f03adeb5bd16a"}
{"id": "00000099", "task": "rating", "text": "这其实也是一个启发,benchmark 要做面向未来的研究——找清楚 benchmark 立项时和实际发布时的时间差。", "spans": [{"unit": "char", "start": 10, "end": 19, "lang": "en"}, {"unit": "char", "start": 35, "end": 44, "lang": "en"}], "emd": 0.3462, "context_domain": "content", "dedup_hash": "sha1:b8bb235fc1cc483660dd89a514e1ae442881a545"}
{"id": "00000100", "task": "rating", "text": "譬如假设要在 25 年的上半年发布一个很难的 benchmark,应该做出预期,让 25 年年中的 SOTA model 的水平只能得到 10 分以下,而现在的 SOTA model 只能得到 1 分左右。", "spans": [{"unit": "char", "start": 23, "end": 32, "lang": "en"}, {"unit": "char", "start": 50, "end": 54, "lang": "en"}, {"unit": "char", "start": 55, "end": 60, "lang": "en"}, {"unit": "char", "start": 81, "end": 85, "lang": "en"}, {"unit": "char", "start": 86, "end": 91, "lang": "en"}], "emd": 0.3506, "context_domain": "content", "dedup_hash": "sha1:4e7ad57eb10c0b65393ca4723ae8683fc75f0f39"}
